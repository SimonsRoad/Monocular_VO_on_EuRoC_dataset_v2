\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Optical Flow based Monocular Visual Odometry}

\author{Jincheng Li\\
{\tt\small jcli40@stanford.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Visual odometry is an active research field in robotics. In this paper, 
   the method of using only optical flow tracked features to recover a 
   monocular camera pose is discussed. The discussed algorithms are 
   experimented on EuRoC MAV dataset and results are analyzed. 
\end{abstract}

%%%%%%%%% BODY TEXT
%-------------------------------------------------------------------------
\section{Introduction}

Localization is a fundamental pre-requisite in robotic systems to enable autonomous 
navigation. There exist various sensors to capture ego motion to help recover past trajectory thus identify robot location, from passive measurement sensors such as wheel-based odometry, GPS, IMU and camera, to active ones like radar, lidar. 

Passive sensors are in general preferable as active sensors might suffer from interference in crowded circumstances. Among all the passive sensors, wheel-based odometry can only provide 1-dimension result while high-accuracy GPS/IMU systems come with expensive price tag. In contrast, camera, which is becoming more and more popular in recent years in robotics systems such as autonomous driving cars, is a very affordable passive sensor yet provides rich information of surrounding environment. 

Taking advantage of the rich information from one or more cameras to estimate ego position and/or, to identify robot trajectory is an active research field called VO (Visual Odometry). VO is an essential part of another active research field called V-SLAM (Visual Simultaneously Localization and Mapping).


%-------------------------------------------------------------------------
\subsection{Visual Odometry}

The term VO won popularity since 2004 in Nister's landmark article \cite{Nistr2004VisualO}. Just like the vehicle wheel odometry incrementally accumulates wheel rotations to estimate ego motion, VO examines images captured by on-board camera(s) to recover relative poses of robot body when the images are taken. Stitching recovered poses incrementally provides the trajectory, as well as location of the moving robot. 

The problem of VO is similar in many ways to SfM (Structure from Motion) and SLAM problems. However, unlike SfM problem which usually deals with huge computation-intensive tasks offline, VO or SLAM system requires real-time responses to direct robot motion. VO can be deemed as a subset of SLAM system, i.e., the localization part. It doesn't need to maintain global structure/map consistency as required and fulfilled by loop closure in SLAM. VO only cares about local consistency.

%-------------------------------------------------------------------------
\subsection{Monocular and Stereo Camera System}

Two popular camera sensor set-ups exist today in VO research: monocular and stereo camera systems, as shown in Fgiure \ref{fig:VOSensorSetup}.


\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth, height=5cm]{latex/MonoCamSystem.png} 
\includegraphics[width=0.7\linewidth, height=5cm]{latex/StereoCamSystem.png}

\caption{VO camera sensor setups. (Upper) Monocular camera sensor captures images at different positions. (Lower) Stereo camera sensor captures two images simultaneously at fixed distance $t$, the baseline.}
\label{fig:VOSensorSetup}
\end{figure}
 
Monocular camera system contains a single camera which captures images sequentially as robot body, where it attaches, moves. In comparison, stereo camera system setup rigidly holds two cameras which usually face same direction in parallel but at fixed distance $t$, called the baseline. 

Although both setups follow same epipolar geometry described in \cite{Hartley2004} to recover relative pose, stereo cameras can restore absolute scale according to baseline distance while monocular system cannot recover from scale ambiguity. Nonetheless, due to limited baseline length in real-life applications, stereo camera setup degenerates to monocular case when object distance $D$ to baseline $t$ ratio 
\begin{equation} \label{eq:1}
ratio=D/t
\end{equation}
increases significantly. Thus this paper focuses on monocular discussion without losing generality. 




%-------------------------------------------------------------------------
\subsection{EuRoC MAV dataset}

The EuRoC MAV dataset \cite{Burri25012016} is used to evaluate the VO algorithm described in this paper. The dataset is chosen because:

\noindent 1) It is newly created in 2015 together with ground truth.

\noindent 2) The camera sensors are global-shuttered thus potential issues with rolling-shuttered cameras are completed avoided. 

\noindent 3) Time-synchronized data from other sensors allow future extension of the project.  

Although EuRoC dataset contains stereo camera images, we will only use cam0 data as the monocular sensor captures in this paper.  


\begin{figure}[h]
\includegraphics[width=0.5\textwidth, inner]{latex/opticalflow_euroc_mav.png}
\caption{OpenCV optical flow features tracking example on EuRoC MAV dataset.}
\label{fig:figure2}
\end{figure}

%------------------------------------------------------------------------
\section{Algorithms Overview}

\begin{table}[t]
\begin{center}
\begin{tabular}{ |c|p{0.8\linewidth}| }
 \hline
 A. & Camera images undistortion assuming calibrated camera. \\ 
 \hline
 B. & Use optical flow tracking to establish 2D-2D feature pointsâ€™ correspondences between consecutive frames. \\ 
 \hline
 C. & Estimate Fundamental or Essential Matrix using epipolar geometry constraints. Use RANSAC \cite{Fischler:1981:RSC:358669.358692} methods to rule out outliers in point-pairs.\\ 
 \hline
 D. & Recover camera pose (rotation R and translation T) from chosen Fundamental or Essential Matrix estimation, minimizing re-projection error to successfully initialize the VO. \\ 
 \hline
 E. & Triangulate 2D feature points to recover feature 3D coordinates in world coordinate system. \\ 
 \hline
 F. & Continue to use optical flow to track features and use triangulation to establish 3D-2D mapping problem to recover follow-up R and T poses using PnP algorithm. \\ 
 \hline
 G. & Camera trajectory can be obtained by stitching recovered camera poses, which is output of the VO program. Compare with ground truth and results analysis. \\
 \hline
\end{tabular}
\end{center}
\caption{Optical flow based Monocular VO steps.}
\label{table:1}
\end{table}

The task of a VO system is to estimate relative poses between consecutive image frames temporally captured by the same camera sensor rigidly mounted on robot body. 

Sequentially concatenating the estimated poses provides the current location and orientation of camera. Since relative pose is known from robot body to camera sensor, the robot location and orientation can be easily calculated from camera ones. Usually the camera center of first selected frame is chosen to be the world coordinate system origin. 

The steps of optical-flow based monocular VO described in this paper are summarized in Table \ref{table:1} in simplest format. Due to time constraint, key elements usually exist in VO systems, such as keyframe maintenance, tracking lost recovery etc. are not included in this paper. 


The key algorithms involved are detailed in next section. 


%------------------------------------------------------------------------
\section{Detailed Technical Approach}


%------------------------------------------------------------------------
\subsection{Image Distortion Correction}

Image frames captured by real-life cameras suffer from lens radial and tangential distortions. Images are undistorted using camera parameters acquired in calibration process, assuming below distortion model \cite{OpenCV_img_undistort}, $k_1, k_2, p_1, p_2$:
\begin{align*} 
x_{radial} &= x(1+k_1 r^2 + k_2 r^4) \\ 
y_{radial} &= y(1+k_1 r^2 + k_2 r^4) \\ 
x_{tangential} &= x + [2 p_1 x y + p_2 (r^2 + 2 x^2)] \\ 
y_{tangential} &= y + [p_1 (r^2 + 2 y^2) + 2 p_2 x y]
\label{eq:2}
\end{align*} 



%------------------------------------------------------------------------
\subsection{Optical Flow based Feature Tracking}\label{optical_flow_subsection}


Optical flow based on Lucas-Kanade algorithm \cite{Lucas-1981-15101} is a popular method to find and track features in consecutive image frames. It is relatively cheap in computation comparing to descriptor based methods, such as SIFT (Sorting Intolerant From Tolerant) \cite{Lowe04distinctiveimage}. 

In this paper, OpenCV optical flow implementations \cite{OpenCV_optical_flow} are used for experiments. Figure \ref{fig:figure2} shows an example of OpenCV optical flow tracking feature points on EuRoC MAV dataset. Outliers are easily found there. 

%-------------------------------------------------------------------------
\subsection{Monocular VO Initialization}

Monocular VO initialization runs only once in a successful odometry recording process. It determines the origin and three axis of world coordinate system (usually set at camera center of very first image frame as illustrated in Figure \ref{fig:figure3}), as well as initial features (X, Y, Z) coordinates in this world system besides the scale in the world system for follow up operations. 

%-------------------------------------------------------------------------
\subsubsection{Fundamental $F$ or Essential Matrix $E$, 2D-2D correspondence}

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{latex/EpipolarConstraint.png}
\caption{Epipolar constraint illustration.}
\label{fig:figure3}
\end{figure}

Let $P$ be a feature point in world system, $p_1, p_2$ be the correspondences of $P$'s image captured on canonical plane of monocular camera sensor at different times. We have the epipolar constraint \cite{Hartley2004}:
\begin{equation} \label{eq:2}
p_2^T F p_1 = 0
\end{equation}
$F$ is the Fundamental Matrix. Since camera is calibrated thus intrinsics $K$ is known, \ref{eq:3} can be re-written using $E$, Essential Matrix:
\begin{equation} \label{eq:3}
p_2^T E p_1 = 0
\end{equation}
where $E=K^T F K$. 

%-------------------------------------------------------------------------
\subsubsection{Extract $R, t$ from $E$ and Triangulation}

Using the algorithm described in section 9.6 of \cite{Hartley2004}, we can extract relative rotation and translation $R, t$, between two camera positions from $E$. We have to triangulate a sample point into 3D space to find the true solution from four solutions provided. 

%-------------------------------------------------------------------------
\subsubsection{Initialization with RANSAC}

Feature correspondences in 2D image, $p_1, p_2$, are provided by optical flow tracker described in \ref{optical_flow_subsection}. We need at least 8 point correspondences to calculate $F$ (normalized 8-pt algorithm \cite{Hartley2004}), or at least 5 point correspondences to calculate $E$ \cite{Nister:2004:ESF:987526.987623}. In this paper we use the linear 8-point algorithm for its simpler implemenation.

We can calculate least squares solution of $F$ as we have more than 8 point feature  correspondences. However, due to existing outliers (easily seen in Figure \ref{fig:figure2}), we'll get erroneous result. Here, we use RANSAC \cite{Fischler:1981:RSC:358669.358692} algorithm to randomly select 8 point correspondences to calculate a candidate $F_i$. Then we calculate $E_i$ according to candidate $F_i$. Pose candidate $R_i, t_i$ follows. 

Finally, using $K, R_i, t_i$, we re-project \emph{ALL} feature correspondence points one by one onto image plane and examine the re-projection error. If error is larger than certain threshold $Thresh$, the feature correspondence pair is deemed as outlier for current candidate $F_i, R_i, t_i$. 
\begin{equation} \label{eq:3}
{err}_{reproj} = d(x_i, \hat{x_i})
\end{equation}
\begin{equation} \label{eq:3}
if ({err}_{reproj} < Thresh):
    {inlier}_{cnt} += 1
\end{equation}
The candidate solution with most inliers stay as final solution. All inlier points are triangulated into world system to get (X,Y,Z) coordinates for future steps. They form the initialized point cloud of features. 


%-------------------------------------------------------------------------
\subsection{3D-2D point-pairs: PnP}


%-------------------------------------------------------------------------
\subsection{Bundle Adjustment}



%------------------------------------------------------------------------
\section{Preliminary Result Analysis}


%------------------------------------------------------------------------
\section{Conclusion and Future Work}


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

